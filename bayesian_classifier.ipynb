{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('traindata.csv', header=None, skiprows=1, usecols=[0,1])\n",
    "\n",
    "#print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['science' 'sports' 'business' 'covid']\n"
     ]
    }
   ],
   "source": [
    "unique_class=file[0].unique()\n",
    "print(unique_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.shape\n",
    "total_count_train=file.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_scaling(text):\n",
    "    step = str.maketrans('', '','0,1,2,3,4,5,6,7,8,9')\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    text=text.translate(step)\n",
    "    text=text.translate(table)\n",
    "    word=text.split()\n",
    "\n",
    "        #converting to lower case letters\n",
    "    words = [w.lower() for w in word]\n",
    "    filtered_words=[]\n",
    "    #filtering the words with removing stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    #print(stop_words) \n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)\n",
    "    words=filtered_words\n",
    "\n",
    "        #portedStemmer\n",
    "    ported_sentence=[]\n",
    "    for w in words:\n",
    "        trial=ps.stem(w)\n",
    "        ported_sentence.append(trial)\n",
    "    words=ported_sentence\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "def text_word_scaling(file):\n",
    "    \n",
    "    class_words={}\n",
    "    class_label_count={}\n",
    "    total_count_class={}\n",
    "    for i in range(80):\n",
    "        clas=file.iloc[i,0]\n",
    "        text=file.iloc[i,1]\n",
    "        words=text_scaling(text)\n",
    "\n",
    "        if clas in class_words:\n",
    "            class_words[clas].extend(words)\n",
    "            class_label_count[clas]+=1\n",
    "            total_count_class[clas]+=len(words)\n",
    "        else:\n",
    "            class_words[clas] = words\n",
    "            class_label_count[clas]=1\n",
    "            total_count_class[clas]=len(words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(class_label_count)\n",
    "    print(total_count_class)\n",
    "    return class_words, class_label_count, total_count_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'science': 20, 'sports': 20, 'business': 19, 'covid': 21}\n",
      "{'science': 200, 'sports': 420, 'business': 348, 'covid': 252}\n"
     ]
    }
   ],
   "source": [
    "#print(class_words)\n",
    "class_words, class_label_count, total_count_class=text_word_scaling(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200, 420, 348, 252]\n",
      "{'science': 0.25, 'sports': 0.25, 'business': 0.2375, 'covid': 0.2625}\n"
     ]
    }
   ],
   "source": [
    "list_count=[]\n",
    "prior_probability={}\n",
    "for k in total_count_class:\n",
    "    list_count.append(total_count_class[k])\n",
    "print(list_count)\n",
    "for k in class_label_count:\n",
    "    prior_probability[k]=class_label_count[k]/total_count_train\n",
    "print(prior_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(words):\n",
    "    word_count = {}\n",
    "    for i in words:\n",
    "        if i in word_count:\n",
    "            word_count[i] += 1\n",
    "        else:\n",
    "            word_count[i] = 1\n",
    "    return word_count\n",
    "\n",
    "def conditional_probability(frequency, count):\n",
    "    class_conditional={}\n",
    "    for k in frequency:\n",
    "        class_conditional[k] = float(frequency[k]/count)\n",
    "    return class_conditional\n",
    "\n",
    "class_conditional_probability={}\n",
    "i=0;\n",
    "for k in class_words:\n",
    "    class_frequency_words=frequency(class_words[k])\n",
    "    conditional_prob = conditional_probability(class_frequency_words, list_count[i])\n",
    "    i+=1\n",
    "    class_conditional_probability[k] = conditional_prob\n",
    "    #print(class_frequency_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(class_conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 2\n"
     ]
    }
   ],
   "source": [
    "test_file = pd.read_csv('testdata.csv', header=None, skiprows=1, usecols=[0,1])\n",
    "x,y = test_file.shape\n",
    "print(x, y)\n",
    "#print(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_find(test_scaled_words, class_conditional_probability, prior_probability):\n",
    "    posterior_distribution={}\n",
    "    probability_feature=0\n",
    "    for train_class in prior_probability:\n",
    "        posterior_probility = 1\n",
    "        for w_test in test_scaled_words:\n",
    "            class_conditional_word_probabilities= class_conditional_probability[train_class]\n",
    "            if w_test not in class_conditional_word_probabilities:\n",
    "                posterior_probility *= 0.001\n",
    "            else:\n",
    "                posterior_probility *= class_conditional_word_probabilities[w_test]\n",
    "        posterior_probility *= prior_probability[train_class]\n",
    "        probability_feature += posterior_probility\n",
    "        posterior_distribution[train_class] = posterior_probility\n",
    "    return posterior_probility, posterior_distribution, probability_feature\n",
    "\n",
    "confidence_list=[]\n",
    "predicted_list=[]\n",
    "    \n",
    "for i in range(x):\n",
    "    text=test_file.iloc[i,1]\n",
    "    test_scaled_words = text_scaling(text)\n",
    "    #print(test_scaled_words)\n",
    "    posterior_probility, posterior_distribution, probability_feature=posterior_find(test_scaled_words,class_conditional_probability, prior_probability)\n",
    "    maximum_probability=-1\n",
    "    for key in posterior_distribution:\n",
    "        posterior_distribution[key] = posterior_distribution[key]/probability_feature\n",
    "        if posterior_distribution[key] > maximum_probability:\n",
    "            max_p_ind = key \n",
    "            maximum_probability = posterior_distribution[key]\n",
    "    confidence_list.append(maximum_probability)\n",
    "    predicted_list.append(max_p_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['science', 'science', 'science', 'science', 'science', 'sports', 'sports', 'sports', 'sports', 'sports', 'business', 'business', 'business', 'business', 'business', 'covid', 'covid', 'covid', 'covid', 'covid']\n"
     ]
    }
   ],
   "source": [
    "print(predicted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999993834670468, 0.9037337995591453, 0.9998325281034195, 0.9999999526960167, 0.879421536056283, 0.999999999891396, 0.9999999999999741, 0.9999999999588761, 0.9976989909947366, 0.9998479790821991, 1.0, 0.9865961475268178, 0.8372377991538786, 0.9952624753945555, 0.9999999999997176, 0.9999819951734475, 0.9999970698650167, 0.9999996650675628, 0.9999999871736334, 0.9999907615262289]\n"
     ]
    }
   ],
   "source": [
    "print(confidence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# continuation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_2 = pd.read_csv('40.csv', header=None, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 1\n"
     ]
    }
   ],
   "source": [
    "x,y=file_2.shape\n",
    "print(x,y)\n",
    "#print(file_2.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['midst', 'covid', 'pandem', 'eat', 'healthi', 'food', 'remain', 'import', 'part', 'maintain', 'health', 'specif', 'food', 'help', 'protect', 'viru', 'nutriti', 'diet', 'boost', 'immun']\n",
      "[['midst', 'covid', 'pandem', 'eat', 'healthi', 'food', 'remain', 'import', 'part', 'maintain', 'health'], ['specif', 'food', 'help', 'protect', 'viru', 'nutriti', 'diet', 'boost', 'immun', 'system', 'help', 'fight', 'symptom'], ['may', 'abl', 'share', 'meal', 'friend', 'love', 'one', 'lot', 'way', 'eat', 'well', 'support', 'health', 'difficult', 'time']]\n"
     ]
    }
   ],
   "source": [
    "total_vocab_list=[]\n",
    "split_words=[]\n",
    "for i in range(x):\n",
    "    text_2=file_2.iloc[i,0]\n",
    "    words=text_scaling(text_2)\n",
    "    split_words.append(words)\n",
    "    total_vocab_list.extend(words)\n",
    "print(total_vocab_list[0:20])\n",
    "print(split_words[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "part-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'midst': 0.0017331022530329288, 'covid': 0.0017331022530329288, 'pandem': 0.0017331022530329288, 'eat': 0.029462738301559793, 'healthi': 0.024263431542461005, 'food': 0.03292894280762565, 'remain': 0.0017331022530329288, 'import': 0.005199306759098787, 'part': 0.0017331022530329288, 'maintain': 0.0017331022530329288, 'health': 0.008665511265164644, 'specif': 0.0017331022530329288, 'help': 0.01733102253032929, 'protect': 0.0034662045060658577, 'viru': 0.0017331022530329288, 'nutriti': 0.0017331022530329288, 'diet': 0.02079722703639515, 'boost': 0.005199306759098787, 'immun': 0.0017331022530329288, 'system': 0.0034662045060658577, 'fight': 0.0017331022530329288, 'symptom': 0.0034662045060658577, 'may': 0.0017331022530329288, 'abl': 0.0017331022530329288, 'share': 0.0017331022530329288, 'meal': 0.008665511265164644, 'friend': 0.0017331022530329288, 'love': 0.005199306759098787, 'one': 0.0034662045060658577, 'lot': 0.0017331022530329288, 'way': 0.005199306759098787, 'well': 0.005199306759098787, 'support': 0.0034662045060658577, 'difficult': 0.0017331022530329288, 'time': 0.0017331022530329288, 'strict': 0.0017331022530329288, 'limit': 0.0034662045060658577, 'stay': 0.005199306759098787, 'unrealist': 0.0017331022530329288, 'thin': 0.0017331022530329288, 'depriv': 0.0017331022530329288, 'rather': 0.0034662045060658577, 'it’': 0.008665511265164644, 'feel': 0.010398613518197574, 'great': 0.0017331022530329288, 'energi': 0.010398613518197574, 'improv': 0.005199306759098787, 'mood': 0.006932409012131715, 'doesn’t': 0.005199306759098787, 'overli': 0.0017331022530329288, 'complic': 0.0017331022530329288, 'overwhelm': 0.0017331022530329288, 'conflict': 0.0017331022530329288, 'nutrit': 0.0017331022530329288, 'advic': 0.0017331022530329288, 'you’r': 0.0034662045060658577, 'alon': 0.0034662045060658577, 'need': 0.005199306759098787, 'balanc': 0.0017331022530329288, 'protein': 0.006932409012131715, 'fat': 0.01559792027729636, 'carbohydr': 0.0034662045060658577, 'fiber': 0.005199306759098787, 'vitamin': 0.005199306759098787, 'miner': 0.0034662045060658577, 'sustain': 0.0017331022530329288, 'bodi': 0.0034662045060658577, 'give': 0.0034662045060658577, 'get': 0.005199306759098787, 'go—and': 0.0017331022530329288, 'keep': 0.0017331022530329288, 'going—whil': 0.0017331022530329288, 'also': 0.008665511265164644, 'cognit': 0.0017331022530329288, 'function': 0.0017331022530329288, 'much': 0.0034662045060658577, 'harm': 0.0017331022530329288, 'peopl': 0.0017331022530329288, 'kidney': 0.0017331022530329288, 'diseas': 0.005199306759098787, 'latest': 0.0017331022530329288, 'research': 0.0017331022530329288, 'suggest': 0.0017331022530329288, 'mani': 0.006932409012131715, 'us': 0.006932409012131715, 'highqual': 0.0017331022530329288, 'especi': 0.005199306759098787, 'age': 0.0034662045060658577, 'bad': 0.0017331022530329288, 'wreck': 0.0017331022530329288, 'increas': 0.0017331022530329288, 'risk': 0.0034662045060658577, 'certain': 0.0017331022530329288, 'good': 0.0017331022530329288, 'brain': 0.0017331022530329288, 'heart': 0.0034662045060658577, 'fact': 0.0017331022530329288, 'fats—such': 0.0017331022530329288, 'omegas—ar': 0.0017331022530329288, 'vital': 0.0034662045060658577, 'physic': 0.0017331022530329288, 'emot': 0.005199306759098787, 'includ': 0.0034662045060658577, 'wellb': 0.0017331022530329288, 'even': 0.005199306759098787, 'trim': 0.0017331022530329288, 'waistlin': 0.0034662045060658577, 'high': 0.0017331022530329288, 'dietari': 0.0017331022530329288, 'grain': 0.0034662045060658577, 'fruit': 0.006932409012131715, 'veget': 0.008665511265164644, 'nut': 0.0017331022530329288, 'bean': 0.0034662045060658577, 'regular': 0.0017331022530329288, 'lower': 0.0017331022530329288, 'stroke': 0.0017331022530329288, 'diabet': 0.0017331022530329288, 'skin': 0.0017331022530329288, 'lose': 0.0017331022530329288, 'weight': 0.0017331022530329288, 'lead': 0.005199306759098787, 'osteoporosi': 0.0017331022530329288, 'enough': 0.0034662045060658577, 'calcium': 0.005199306759098787, 'contribut': 0.0017331022530329288, 'anxieti': 0.0034662045060658577, 'depress': 0.0034662045060658577, 'sleep': 0.0017331022530329288, 'difficulti': 0.0017331022530329288, 'whatev': 0.0017331022530329288, 'gender': 0.0017331022530329288, 'calciumrich': 0.0017331022530329288, 'deplet': 0.0017331022530329288, 'magnesium': 0.0017331022530329288, 'k': 0.0017331022530329288, 'job': 0.0017331022530329288, 'body’': 0.0017331022530329288, 'main': 0.0017331022530329288, 'sourc': 0.0017331022530329288, 'come': 0.0017331022530329288, 'complex': 0.0017331022530329288, 'unrefin': 0.0017331022530329288, 'carb': 0.0034662045060658577, 'whole': 0.0017331022530329288, 'sugar': 0.010398613518197574, 'refin': 0.0017331022530329288, 'cut': 0.005199306759098787, 'back': 0.005199306759098787, 'white': 0.0017331022530329288, 'bread': 0.0017331022530329288, 'pastri': 0.0017331022530329288, 'starch': 0.0017331022530329288, 'prevent': 0.0017331022530329288, 'rapid': 0.0017331022530329288, 'spike': 0.0017331022530329288, 'blood': 0.0017331022530329288, 'fluctuat': 0.0017331022530329288, 'buildup': 0.0017331022530329288, 'around': 0.0017331022530329288, 'switch': 0.0034662045060658577, 'noth': 0.0017331022530329288, 'proposit': 0.0017331022530329288, 'don’t': 0.006932409012131715, 'perfect': 0.0017331022530329288, 'complet': 0.0017331022530329288, 'elimin': 0.0034662045060658577, 'enjoy': 0.0017331022530329288, 'chang': 0.0034662045060658577, 'everyth': 0.0017331022530329288, 'once—that': 0.0017331022530329288, 'usual': 0.0017331022530329288, 'cheat': 0.0017331022530329288, 'new': 0.0034662045060658577, 'plan': 0.0034662045060658577, 'think': 0.0017331022530329288, 'number': 0.0017331022530329288, 'small': 0.0034662045060658577, 'manag': 0.0017331022530329288, 'steps—lik': 0.0017331022530329288, 'ad': 0.005199306759098787, 'salad': 0.0034662045060658577, 'day': 0.0017331022530329288, 'becom': 0.0034662045060658577, 'habit': 0.0017331022530329288, 'continu': 0.0017331022530329288, 'add': 0.0034662045060658577, 'choic': 0.0034662045060658577, 'cook': 0.0034662045060658577, 'home': 0.0017331022530329288, 'take': 0.0017331022530329288, 'charg': 0.0017331022530329288, 'better': 0.0034662045060658577, 'monitor': 0.0017331022530329288, 'exactli': 0.0017331022530329288, 'goe': 0.0017331022530329288, 'you’ll': 0.0034662045060658577, 'fewer': 0.0017331022530329288, 'calori': 0.0034662045060658577, 'avoid': 0.0017331022530329288, 'chemic': 0.0017331022530329288, 'addit': 0.0017331022530329288, 'unhealthi': 0.006932409012131715, 'packag': 0.0034662045060658577, 'takeout': 0.0017331022530329288, 'leav': 0.0017331022530329288, 'tire': 0.0017331022530329288, 'bloat': 0.0017331022530329288, 'irrit': 0.0017331022530329288, 'exacerb': 0.0017331022530329288, 'stress': 0.0034662045060658577, 'replac': 0.0034662045060658577, 'altern': 0.0017331022530329288, 'danger': 0.0017331022530329288, 'tran': 0.0017331022530329288, 'fri': 0.0034662045060658577, 'chicken': 0.0017331022530329288, 'grill': 0.0034662045060658577, 'salmon': 0.0017331022530329288, 'make': 0.0034662045060658577, 'posit': 0.0017331022530329288, 'differ': 0.0017331022530329288, 'awar': 0.0017331022530329288, 'what’': 0.0017331022530329288, 'manufactur': 0.0017331022530329288, 'often': 0.0034662045060658577, 'hide': 0.0017331022530329288, 'larg': 0.0017331022530329288, 'amount': 0.0034662045060658577, 'claim': 0.0017331022530329288, 'healthier': 0.0034662045060658577, 'junk': 0.0017331022530329288, 'like': 0.0017331022530329288, 'uncomfort': 0.0017331022530329288, 'nauseou': 0.0017331022530329288, 'drain': 0.0017331022530329288, 'drink': 0.0017331022530329288, 'plenti': 0.0034662045060658577, 'water': 0.0034662045060658577, 'flush': 0.0017331022530329288, 'wast': 0.0017331022530329288, 'product': 0.0017331022530329288, 'toxin': 0.0017331022530329288, 'yet': 0.0017331022530329288, 'go': 0.0017331022530329288, 'life': 0.0017331022530329288, 'dehydrated—caus': 0.0017331022530329288, 'tired': 0.0017331022530329288, 'low': 0.0034662045060658577, 'headach': 0.0017331022530329288, 'common': 0.0017331022530329288, 'mistak': 0.0017331022530329288, 'thirst': 0.0017331022530329288, 'hunger': 0.0034662045060658577, 'hydrat': 0.0017331022530329288, 'moder': 0.005199306759098787, 'essenc': 0.0017331022530329288, 'mean': 0.006932409012131715, 'satisfi': 0.005199306759098787, 'end': 0.0017331022530329288, 'stuf': 0.0017331022530329288, 'less': 0.0017331022530329288, 'bacon': 0.0017331022530329288, 'breakfast': 0.0017331022530329288, 'week': 0.0017331022530329288, 'exampl': 0.0017331022530329288, 'could': 0.0017331022530329288, 'consid': 0.0017331022530329288, 'follow': 0.0034662045060658577, 'lunch': 0.0017331022530329288, 'dinner—but': 0.0017331022530329288, 'box': 0.0017331022530329288, 'donut': 0.0017331022530329288, 'sausag': 0.0017331022530329288, 'pizza': 0.0017331022530329288, 'front': 0.0017331022530329288, 'tv': 0.0017331022530329288, 'comput': 0.0017331022530329288, 'mindless': 0.0017331022530329288, 'over': 0.0017331022530329288, 'control': 0.0017331022530329288, 'alway': 0.0017331022530329288, 'turn': 0.0017331022530329288, 'reliev': 0.0017331022530329288, 'cope': 0.0017331022530329288, 'unpleas': 0.0017331022530329288, 'sad': 0.0017331022530329288, 'loneli': 0.0017331022530329288, 'boredom': 0.0017331022530329288, 'nutrient': 0.0017331022530329288, 'dens': 0.0017331022530329288, 'pack': 0.0017331022530329288, 'antioxid': 0.0017331022530329288, 'focu': 0.0017331022530329288, 'recommend': 0.0017331022530329288, 'daili': 0.0017331022530329288, 'least': 0.0017331022530329288, 'five': 0.0017331022530329288, 'serv': 0.0017331022530329288, 'natur': 0.0034662045060658577, 'fill': 0.0017331022530329288, 'sweet': 0.006932409012131715, 'tooth': 0.0017331022530329288, 'vegetables—such': 0.0017331022530329288, 'carrot': 0.0017331022530329288, 'beet': 0.0017331022530329288, 'potato': 0.0017331022530329288, 'yam': 0.0017331022530329288, 'onion': 0.0034662045060658577, 'bell': 0.0017331022530329288, 'pepper': 0.0017331022530329288, 'squash—add': 0.0017331022530329288, 'reduc': 0.0017331022530329288, 'crave': 0.0017331022530329288, 'green': 0.0017331022530329288, 'broccoli': 0.0017331022530329288, 'brussel': 0.0017331022530329288, 'sprout': 0.0017331022530329288, 'asparagu': 0.0017331022530329288, 'instead': 0.0017331022530329288, 'boil': 0.0017331022530329288, 'steam': 0.0034662045060658577, 'side': 0.0017331022530329288, 'tri': 0.0017331022530329288, 'roast': 0.0017331022530329288, 'pan': 0.0017331022530329288, 'chili': 0.0017331022530329288, 'flake': 0.0017331022530329288, 'garlic': 0.0017331022530329288, 'shallot': 0.0017331022530329288, 'mushroom': 0.0017331022530329288, 'plain': 0.0017331022530329288, 'veggi': 0.0017331022530329288, 'quickli': 0.0017331022530329288, 'bland': 0.0017331022530329288, 'tast': 0.0017331022530329288, 'dish': 0.0017331022530329288}\n"
     ]
    }
   ],
   "source": [
    "freq_vocab=frequency(total_vocab_list)\n",
    "n=len(total_vocab_list)\n",
    "prior_2={}\n",
    "for k in freq_vocab:\n",
    "    prior_2[k]=freq_vocab[k]/n\n",
    "print(prior_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
